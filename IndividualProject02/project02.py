#!/usr/bin/env python
# coding: utf-8

# # Individual Project 02 Yifan(Crystal)CAI

# Please use functions to subdivide each section in this project.  E.g., please create a function called q2 that solves all of part (2) etc.  In the end, there should be a one file called “project.py” or “project.java”.  Jupyter notebooks are not accepted.  (I want to be able to run your entire code in full at once…) <br>
# Please compress your code file along with all downloaded HTML files into a single zip file and submit that.

# ## Selenium:  The Bored Ape Yacht Club

# According to Wikipedia, “The Bored Ape Yacht Club is a non-fungible token (NFT) collection built on the Ethereum blockchain.  The collection features profile pictures of cartoon apes that are procedurally generated by an algorithm.  As of 2022, Bored Ape Yacht Club’s parent company, Yuga Labs, is valued at US 4 billion.  This is due in large part to the sales of the Bored Ape Yacht Club NFT collection totaling over US$1 billion.  Various celebrities have purchased these non-fungible tokens, including Justin Bieber, Snoop Dogg, Gwyneth Paltrow and others.”
# 
# (1)  (No programming yet,) go to https://opensea.io/collection/boredapeyachtclub. and select all apes with “Solid gold” fur and sort them “Price high to low” .  Use the URL for the subsequent coding.

# In[1]:


# Take the url for subsequent coding 
url = "https://opensea.io/collection/boredapeyachtclub?search[sortAscending]=false&search[stringTraits][0][name]=Fur&search[stringTraits][0][values][0]=Solid%20Gold"


# (2)  Using Python or Java, write code that uses Selenium to access the URL from (1), click on each of the top-8 most expensive Bored Apes, and store the resulting details page to disk, “bayc_[N].htm” (replace [N] with the ape number).

# In[2]:


# !pip3 install -U selenium
# !pip3 install webdriver-manager


# In[3]:


import selenium
import time
import requests
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options


# In[4]:


# Function 
def ClickAndDownloadApes():
    # Setup of Selenium
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    driver.implicitly_wait(10)
    driver.set_script_timeout(150)
    driver.set_page_load_timeout(10)

    # Click on each of the top 8 apes and download the pages 
    driver.get(url)
    
    ape_links = []
    for i in range(0,8):
        # Get the link for each ape
        ape = driver.find_elements(By.CSS_SELECTOR,  "a.Asset--anchor")[i]
        ape_link = ape.get_attribute('href')
        ape_links.append(ape_link)
    driver.quit()
    
    for ape_link in ape_links:
        # Setup of Selenium
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
        driver.implicitly_wait(10)
        driver.set_script_timeout(150)
        driver.set_page_load_timeout(10)
        
        # Click on the page 
        driver.get(ape_link)
        time.sleep(5)
        
        # Download the page 
        ape_page = driver.page_source
        folder_path = "/Users/crystal/Desktop/IndividualProject02/bayc/"
        file_name =  f"bayc_[{ape_link.split('/')[-1]}].htm"
        file_path = folder_path + file_name
        try:
            with open(file_path, "w") as file:
                file.write(ape_page)
                print(f"{file_name} has been downloaded")
        except:
            print(f"Error: {file_name} cannot be downloaded")
    driver.quit()


# In[5]:


# Deploy the function
ClickAndDownloadApes()


# ## MongoDB

# (3)  Write code that goes through all 8 htm files downloaded in (2) and stores each ape’s name (its number) and all its attributes in a document inside a MongoDB collection called “bayc”.

# In[6]:


import pymongo
import os
import re
from pymongo import MongoClient
from bs4 import BeautifulSoup


# In[7]:


#Function 
def StoreApes():
    #Connet to MongoDB
    username = 'cyf08050716'
    password = 'wyz0522'
    database = 'Apes'
    connection_string = f'mongodb+srv://{username}:{password}@cluster0.avsc5qq.mongodb.net/{database}?appName=mongosh+1.7.1'
    client = MongoClient(connection_string)
    # Read in all downloaded htm file
    folder_path = "/Users/crystal/Desktop/IndividualProject02/bayc/"
    file_names = os.listdir(folder_path)
    htm_files = [f for f in file_names if f.endswith('htm')]
    
    # Scrap each htm file 
    ape_dict = {}
    for htm_file in htm_files:
        ape_number = re.search(r'\[(\d+)\]', htm_file).group(1)
        htm = f'file://{os.path.abspath(os.path.join(folder_path, htm_file))}'
        
        # Setup of Selenium
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
        driver.implicitly_wait(10)
        driver.set_script_timeout(150)
        driver.set_page_load_timeout(10)
        driver.get(htm)
        
        # Scrape properties of apes
        property_types = driver.find_elements(By.XPATH, '//div[@class="Property--type"]')
        properties = driver.find_elements(By.XPATH, '//div[@class="Property--value"]')
        property_list = []
        for i in range(len(property_types)):
            property_list.append({property_types[i].text:properties[i].text})
            temp_dict = {'Ape':ape_number}
        for item in property_list:
            temp_dict.update(item)
        ape_dict[ape_number] = temp_dict
        driver.quit()
    # Insert dictionary into MongoDB collection
    db = client['Apes']
    collection = db['bayc']
    for ape_number, properties in ape_dict.items():
        collection.update_one({'Ape':ape_number}, {'$set':properties}, upsert = True)


# In[8]:


# Deploy the function
StoreApes()


# ## Regular Webscraping

# (4)  Yellow Pages uses GET requests for its search.  Using plain Python or Java (no Selenium), write a program that searches on yellowpages.com for the top 30 “Pizzeria” in San Francisco (no need to verify that the shop is actually selling pizzas, just search for “Pizzeria”, top 30 shops according to YP's "Default" sorting).  Save each search result page to disk, “sf_pizzeria_search_page.htm”.

# In[9]:


import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
session = requests.Session()
retry = Retry(connect=3, backoff_factor=0.5)
adapter = HTTPAdapter(max_retries=retry)
session.mount('http://', adapter)
session.mount('https://', adapter)


# In[10]:


def Top30Pizzeria():
    headers = {"User-Agent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5)AppleWebKit/605.1.15 (KHTML, like Gecko)Version/12.1.1 Safari/605.1.15'}
    # Go to yellowpages.com and search for 'Pizzeria'
    url_yellow = 'https://www.yellowpages.com'
    url_pizzeria = url_yellow+'/search?search_terms=Pizzeria'
    requests.adapters.DEFAULT_RETRIES = 5 
    page = requests.get(url_pizzeria, headers = headers, verify = False)
    soup = BeautifulSoup(page.content, "html.parser")
    items = soup.find_all('a', {'class' : 'business-name', 'data-analytics':'{"target":"name","feature_click":""}'})
    links = []
    for item in items:
        link = url_yellow + item.get('href')
        links.append(link)
    #Set up folder where I hope to download my pages into
    folder_path = "/Users/crystal/Desktop/IndividualProject02/sf_pizzeria/"
    #For loop to go through the list of links to download pages
    for i in range(1, len(links)+1):
        link = links[i-1]
        file_name = f"sf_pizzeria_search_page_{str(i).zfill(2)}.htm"
        file_path = folder_path + file_name
        
        try:
            html = requests.get(link, timeout=20, headers = headers).text
            with open(file_path, "w") as file:
                file.write(html)
                #Signal me when a file has been successfully downloaded
                print(f"{file_name} has been downloaded")
        except:
            print(f"Error: {file_name} cannot be downloaded")


# In[11]:


Top30Pizzeria()


# (5)  Using Python or Java, write code that opens the search result page saved in (4) and parses out all shop information (search rank, name, linked URL [this store’s YP URL], star rating If It Exists, number of reviews IIE, TripAdvisor rating IIE, number of TA reviews IIE, “$” signs IIE, years in business IIE, review IIE, and amenities IIE).  Please be sure to skip all “Ad” results.

# In[12]:


def ParseTop30Pizzeria():
    import re
    headers = {"User-Agent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5)AppleWebKit/605.1.15 (KHTML, like Gecko)Version/12.1.1 Safari/605.1.15'}
    # Search into 'Pizzeria'
    search_term = 'Pizzeria'
    url_pizzeria = f'https://www.yellowpages.com/search?search_terms={search_term}'

    # Load the website 
    requests.adapters.DEFAULT_RETRIES = 5 
    page = requests.get(url_pizzeria, headers = headers, verify = False)
    soup = BeautifulSoup(page.content, "html.parser")

    # Find all research results 
    results = soup.find_all('div', {'class':'result'})

    # Loop over each result and extract the information 
    for i, result in enumerate(results[1:31]):
        # Rank
        search_rank = i + 1 

        # Name 
        name = result.find('a',{'class':'business-name'}).get_text().strip()

        # YP URL 
        url_yellow = 'https://www.yellowpages.com'
        url = result.find('a', {'class':'business-name'})['href']
        yp_url = url_yellow+url

        # Star Rating 
        star = result.find('div', {'class':'result-rating'})
        if star is not None :
            star_rating = star.get('class')[1]
            #.replace('stars_', '')

        # Number of reviews
        num_reviews = result.find('span',{'class':'result-rating'})
        if num_reviews is not None:
            num_reviews = int(re.findall(r'\d+', num_reviews.get_text())[0])

        # Tripadvisor rating and number of TA reviews
        tripadvisor = result.find('div', {'class':'rating'})
        if tripadvisor is not None:
            t = tripadvisor['data-tripadvisor'].text
            ta_rating = float(json.loads(t)['rating'])
            num_ta_review = int(json.loads(t)['count'])

        # $ signs
        dollar = result.find('div', {'class':'price-range'})
        if dollar is not None:
            dollar_sign = dollar.get_text().strip()

        # Years in business
        years_in_business = result.find('div',{'class':'years-in-business'})
        if years_in_business is not None:
            years_in_business =  years_in_business.get_text().strip()
            years = int(re.findall(r'\d+', years_in_business)[0])

        # Review
        review = result.find('p',{'class':'body with-avatar'})
        if review is not None :
            review = review.get_text().strip()

        # Amenities
        amenities = result.find('div', {'class':'amenities'})
        if amenities is not None:
            amenities = [a.get_text().strip() for a in amenities.find_all('div')]
            amenity = re.split(r'(?<=[a-z])(?=[A-Z])', amenities[0])

        # Print results
        print('='*100)
        print(f'Search Rank: {search_rank}')
        print(f'Name: {name}')
        print(f'YP URL: {yp_url}')
        if star is not None:
            print(f'Star Rating: {star_rating}')
        if num_reviews is not None:
            print(f'Number of Reviews: {num_reviews}')
        if tripadvisor is not None:
            print(f'TripAdvisor Rating: {ta_rating}')
            print(f'Number of TripAdvisor Reviews: {num_ta_review}')
        if dollar is not None :
            print(f'Signs: {dollar_sign}')
        if years_in_business is not None :    
            print(f'Years in Business: {years}')
        if review is not None :    
            print(f'Review: {review}')
        if amenities is not None :    
            print(f'Amenities: {amenity}')   


# In[13]:


ParseTop30Pizzeria()


# ## MongoDB
# (6)  Copy your code from (5).  Modify the code to create a MongoDB collection called “sf_pizzerias” that stores all the extracted shop information, one document for each shop.

# In[14]:


# Connect to MongoDB
username = 'cyf08050716'
password = 'wyz0522'
database = 'Pizzeria'
connection_string = f'mongodb+srv://{username}:{password}@cluster0.avsc5qq.mongodb.net/{database}?appName=mongosh+1.7.1'
client = MongoClient(connection_string)


# In[15]:


def StoreTop30Pizzeria():
    import re
    headers = {"User-Agent": 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5)AppleWebKit/605.1.15 (KHTML, like Gecko)Version/12.1.1 Safari/605.1.15'}
    # Set up to collection and database
    db = client['Pizzeria']
    collection = db['sf_pizzerias']
    
    # Search into 'Pizzeria'
    search_term = 'Pizzeria'
    url_pizzeria = f'https://www.yellowpages.com/search?search_terms={search_term}'

    # Load the website 
    requests.adapters.DEFAULT_RETRIES = 5 
    page = requests.get(url_pizzeria, headers = headers, verify = False)
    soup = BeautifulSoup(page.content, "html.parser")

    # Find all research results 
    results = soup.find_all('div', {'class':'result'})
    
    # Create a dictionary for each pizzeria :
    pizzeria = {}

    # Loop over each result and extract the information 
    for i, result in enumerate(results[1:31]):
        # Rank
        search_rank = i + 1 
        pizzeria['search_rank'] = search_rank

        # Name 
        name = result.find('a',{'class':'business-name'}).get_text().strip()
        pizzeria['name'] = name

        # YP URL 
        url_yellow = 'https://www.yellowpages.com'
        url = result.find('a', {'class':'business-name'})['href']
        yp_url = url_yellow+url
        pizzeria['yp_url'] = yp_url
        
        # Star Rating 
        star = result.find('div', {'class':'result-rating'})
        if star is not None :
            star_rating = star.get('class')[1]
            pizzeria['star_rating'] = star_rating 

        # Number of reviews
        num_reviews = result.find('span',{'class':'result-rating'})
        if num_reviews is not None:
            num_reviews = int(re.findall(r'\d+', num_reviews.get_text())[0])
            pizzeria['num_reviews'] = num_reviews

        # Tripadvisor rating and number of TA reviews
        tripadvisor = result.find('div', {'class':'rating'})
        if tripadvisor is not None:
            t = tripadvisor['data-tripadvisor'].text
            ta_rating = float(json.loads(t)['rating'])
            pizzeria['ta_rating'] = ta_rating 
            num_ta_review = int(json.loads(t)['count'])
            pizzeria['num_ta_review'] = num_ta_review

        # $ signs
        dollar = result.find('div', {'class':'price-range'})
        if dollar is not None:
            dollar_sign = dollar.get_text().strip()
            pizzeria['dollar_sign'] = dollar_sign

        # Years in business
        years_in_business = result.find('div',{'class':'years-in-business'})
        if years_in_business is not None:
            years_in_business =  years_in_business.get_text().strip()
            years = int(re.findall(r'\d+', years_in_business)[0])
            pizzeria['years_in_business'] = years

        # Review
        review = result.find('p',{'class':'body with-avatar'})
        if review is not None :
            review = review.get_text().strip()
            pizzeria['review'] = review

        # Amenities
        amenities = result.find('div', {'class':'amenities'})
        if amenities is not None:
            amenities = [a.get_text().strip() for a in amenities.find_all('div')]
            amenity = re.split(r'(?<=[a-z])(?=[A-Z])', amenities[0])
            pizzeria['amenities'] = amenity
        
        # Save pizzeria information to MongoDB
        collection.update_one({'name':pizzeria['name']}, {'$set':pizzeria}, upsert = True)


# In[16]:


StoreTop30Pizzeria()


# ## Parsing

# (7)  Write code that reads all URLs stored in “sf_pizzerias” and download each shop page.  Store the page to disk, “sf_pizzerias_[SR].htm” (replace [SR] with the search rank).

# In[17]:


def ReadPizzerias():
    import os
    db = client['Pizzeria']
    collection = db['sf_pizzerias']
    folder_path = "/Users/crystal/Desktop/IndividualProject02/q7_htmls/"
    for doc in collection.find():
        search_rank = doc['search_rank']
        url = doc['yp_url']
        response = requests.get(url)
        html = response.content

        file_name = f"sf_pizzerias_{search_rank}.htm"
        file_path = folder_path + file_name

        try : 
            with open(file_path, "wb") as file:
                file.write(html)
                print(f"{file_name} has been downloaded")
        except :
            print(f"Error: {file_name} cannot be downloaded")


# In[18]:


ReadPizzerias()


# (8)  Write code that reads the 30 shop pages saved in (7) and parses each shop’s address, phone number, and website. 

# In[21]:


def ReadandParsePizzerias():
    import os
    folder_path = "/Users/crystal/Desktop/IndividualProject02/q7_htmls/"

    for i in range(1,30):
        file_name = f"sf_pizzerias_{str(i)}.htm"
        file_path = folder_path+file_name
        try :
            with open(file_path, "r") as file:
                html = file.read()
                soup = BeautifulSoup(html, "html.parser")

                # Phone 
                phone = soup.find('p', {'class':'phone'})
                phone_number = phone.get_text().strip().replace('Phone:  ','')

                # Address
                add = phone.find_next_sibling('p')
                address = add.text.strip().replace('Address: ', '')

                # Website 
                web = soup.find('p',{'class':'website'})
                if web is not None:
                    website = web.text.strip().replace('Website: ','')
                else :
                    website = None

                print(f"sf_pizzeria {i}:\nPhone Number: {phone_number}\nAddress: {address}\nWebsite: {website}\n")
        except :
            print(f"Error: {file_name}")


# In[22]:


ReadandParsePizzerias()


# ## API

# (9)  Sign up for a free account with https://positionstack.com/.  Copy your code from (8).  Modify the code to query each shop address’ geolocation (long, lat).  Update each shop document on the MongoDB collection “sf_pizzerias” to contain the shop’s address, phone number, website, and geolocation.

# In[23]:


# Connect to MongoDB
username = 'cyf08050716'
password = 'wyz0522'
database = 'Pizzeria'
connection_string = f'mongodb+srv://{username}:{password}@cluster0.avsc5qq.mongodb.net/{database}?appName=mongosh+1.7.1'
client = MongoClient(connection_string)


# In[24]:


def LocationUpdate():
    import os
    folder_path = "/Users/crystal/Desktop/IndividualProject02/q7_htmls/"

    db = client['Pizzeria']
    collection = db['sf_pizzerias']
    pizzeria_update={}
    
    for i in range(1,30):
        file_name = f"sf_pizzerias_{str(i)}.htm"
        file_path = folder_path+file_name
        try : 
            with open(file_path, "r") as file:
                html = file.read()
                soup = BeautifulSoup(html, "html.parser")
                
                # Name 
                name = soup.find('h1', {'class':'dockable business-name'}).text
                
                # Phone 
                phone = soup.find('p', {'class':'phone'})
                phone_number = phone.get_text().strip().replace('Phone:  ','')
                pizzeria_update['phone'] = phone_number

                # Address
                add = phone.find_next_sibling('p')
                address = add.text.strip().replace('Address: ', '')
                pizzeria_update['address'] = address
                

                # Website 
                web = soup.find('p',{'class':'website'})
                if web is not None:
                    website = web.text.strip().replace('Website: ','')
                    pizzeria_update['website'] = website
                else :
                    website = None
                    pizzeria_update['website'] = None
               
                # Geo_Location 
                api_key = 'f15b80c19e1a22ff67ff6d5acdae80ca'
                base_url = 'http://api.positionstack.com/v1/forward'
                params = {'access_key': api_key, 'query':address}
                response = requests.get(base_url, params = params)
                geo_data = response.json()
                if geo_data['data']:
                    location = geo_data['data'][0]
                    long = location['longitude']
                    pizzeria_update['longitude'] = long
                    lat = location['latitude']
                    pizzeria_update['latitude'] = lat
                else:
                    pizzeria_update['longitude'] = None
                    pizzeria_update['latitude'] = None
                
                collection.update_one({'name':name}, {'$set':pizzeria_update}, upsert = True)
        except :
            print(f"Error: {file_name}")


# In[25]:


LocationUpdate()

